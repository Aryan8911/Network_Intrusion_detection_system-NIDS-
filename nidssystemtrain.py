# -*- coding: utf-8 -*-
"""nidssystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124uwbhUlbj5voObqRa4hUf0MyhElA15G
"""

# Step 1: Install required libraries
!pip install scapy pandas scikit-learn

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib
from google.colab import files

def train_and_save_model(file_name, model_name):
    # Load the dataset
    df = pd.read_parquet(file_name)

    # Display the first few rows to understand the dataset structure
    print(f"Dataset: {file_name}")
    print(df.head())

    # Use 'label' as the label column and drop it from features
    X = df.drop(columns=['label'])  # Features
    y = df['label']  # Labels

    # Convert categorical variables to numerical format using one-hot encoding
    X = pd.get_dummies(X)

    # Split dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model (RandomForest in this case)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))

    # Save the trained model to a file
    joblib.dump(model, model_name)
    print(f"Model saved as {model_name}")

# Train on the external server dataset
train_and_save_model('/content/cidds-001-externalserver.parquet', 'nids_model_external.pkl')

# Train on the OpenStack dataset
train_and_save_model('/content/cidds-001-openstack.parquet', 'nids_model_openstack.pkl')

# Download the saved models to your local machine
files.download('nids_model_external.pkl')
files.download('nids_model_openstack.pkl')

# Import necessary libraries
import joblib
from scapy.all import sniff
import pandas as pd
import numpy as np

# Load the trained models
model_external = joblib.load('nids_model_external.pkl')
model_openstack = joblib.load('nids_model_openstack.pkl')

# Function to convert packet data into the format the model expects
def preprocess_packet(packet):
    # Create a DataFrame from the packet
    data = {
        'duration': packet.time,  # Adjust based on the packet attributes
        'proto': packet.proto,     # Example: 'TCP' or 'UDP'
        'packets': len(packet),    # Number of packets in the captured data
        'bytes': len(bytes(packet)), # Length of the packet in bytes
        # Add other relevant fields from the packet if needed
    }

    # Convert to DataFrame
    df = pd.DataFrame([data])

    # One-hot encode categorical variables
    df = pd.get_dummies(df)

    # Ensure all features used in the training are present in the new data
    for col in model_external.feature_importances_:
        if col not in df.columns:
            df[col] = 0
    df = df.reindex(columns=model_external.feature_importances_, fill_value=0)

    return df

# Packet callback function to process each packet
def packet_callback(packet):
    # Preprocess the packet to match the model input
    processed_packet = preprocess_packet(packet)

    # Predict using the external model
    prediction_external = model_external.predict(processed_packet)

    # Predict using the OpenStack model
    prediction_openstack = model_openstack.predict(processed_packet)

    # Print or log the predictions
    print(f"External Model Prediction: {prediction_external}, OpenStack Model Prediction: {prediction_openstack}")

# Start sniffing packets (replace 'eth0' with the appropriate interface)
sniff(iface='eth0', prn=packet_callback, store=False)